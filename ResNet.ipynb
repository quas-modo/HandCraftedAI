{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef31e5c732e78a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5c0bcff7936eeae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU())\n",
    "    self.conv2 = nn.Sequential(\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU())\n",
    "    self.downsample = downsample\n",
    "    self.relu = nn.ReLU()\n",
    "    self.out_channels = out_channels\n",
    "  \n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    out = self.conv1(x)\n",
    "    out = self.conv2(out)\n",
    "    if self.downsample:\n",
    "      residual = self.downsample(x)\n",
    "    out += residual\n",
    "    out = self.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "  def __init__(self, block, layers, num_classes = 10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.inplanes = 64\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "    self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "    self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "    self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "    self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "    self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, blocks, stride=1):\n",
    "      downsample = None\n",
    "      if stride != 1 or self.inplanes != planes:\n",
    "          \n",
    "          downsample = nn.Sequential(\n",
    "              nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "              nn.BatchNorm2d(planes),\n",
    "          )\n",
    "      layers = []\n",
    "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "      self.inplanes = planes\n",
    "      for i in range(1, blocks):\n",
    "          layers.append(block(self.inplanes, planes))\n",
    "\n",
    "      return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ff36cf9f67f492"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 初始卷积层\n",
    "在残差块之前，ResNet 通常开始于一个单独的卷积层，这个层用于初步提取特征：\n",
    "\n",
    "卷积：使用较大的卷积核（如7x7或者较大的3x3），较大的步长（通常是2），和适当的填充来保持空间尺寸。这一步是为了从原始图像中提取粗略的特征，并减少特征图的尺寸。\n",
    "批量归一化（Batch Normalization）：归一化处理帮助加快收敛速度，同时使模型更加稳定。\n",
    "ReLU激活函数：为了引入非线性，使得网络能够学习更复杂的模式。\n",
    "最大池化：可选的最大池化步骤进一步减小空间尺寸，这在处理非常大的图像时非常有用。\n",
    "## 2. 残差块\n",
    "残差块是ResNet的核心，每个块包括以下几个组件：\n",
    "\n",
    "主路径：包含两个或三个卷积层，每个卷积层后面通常跟着批量归一化和ReLU激活函数。这些卷积层通常使用较小的3x3或1x1卷积核。\n",
    "跳跃连接（Skip Connection）：将输入直接加到块的输出上。如果输入和输出的维度不匹配，可能需要通过一个1x1的卷积来调整通道数或使用步长来改变空间维度。\n",
    "激活函数：在将跳跃连接的结果与主路径输出相加后，通常会应用一个非线性激活函数（如ReLU）。\n",
    "\n",
    "## 3. 层间连接\n",
    "每一组残差块后面通常有一个下采样步骤，用于减少特征图的空间尺寸并增加通道数，从而让网络逐渐学习更高级的特征表示。\n",
    "这通常通过在残差块的第一个卷积层中设置较大的步长（如2），或者在跳跃连接中使用1x1卷积层实现。\n",
    "\n",
    "## 4.结尾层\n",
    "自适应平均池化（Adaptive Average Pooling）：在网络的最后，通常使用自适应平均池化层将特征图的空间尺寸降为1x1，这样每个通道只有一个数值，总结了整个特征图的信息。\n",
    "全连接层：将池化后的特征图展平并通过一个或多个全连接层来进行最终的分类或其他任务。\n",
    "输出：最后的输出层根据任务的不同，可能是一个分类层（使用softmax激活函数）或者其他类型的输出。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6037dd8c3be6257a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
